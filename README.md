CIS 4130 Big Data Technologies â€“ Semester Project
For this semester-long project, students will build a complete machine learning pipeline that incorporates big data technologies using a cloud infrastructure.  The project is split into 7 milestones that will be due throughout the semester.  The project must be original (e.g., not copied from a previous semester, competition or other source). However, code examples can be referenced (proper citations required).
To begin, create a Project Document in MS Word, Google Docs, etc. Create a cover page with CIS 4130, your name and e-mail address on it.  For each milestone, continue adding pages to this document throughout the semester.  For example, when turning in Milestone 3, include everything you wrote about in Milestones 1 and 2.

Milestone 1 Due 9/8/2023 (10 points)
Proposal: Research and find a data set larger than 10 GB (should be free, open source etc.)  OR  describe a plan to collect at least 10 GB of data (e.g., from Twitter or other API).    Some suggestions for data sets include Kaggle, DataHub.io, Data.gov, Open Data on AWS, UCI Machine Learning Repository, NYC OpenData, and Google Public Datasets. Try and pick a data set that aligns with your personal interests. Do not pick data sets that are mostly images, videos or audio (unless you have prior experience working with these types of data).
Write up a brief 1 page proposal that includes a description of the data set, URL/Location for downloading the data, the data set attributes (columns), and a description of what you intend to model, predict, forecast, etc. using the data set.

Milestone 2 Due 9/29/2023 (15 points)
Data Acquisition: Download or collect the data into a bucket on Amazon S3 (or in an AWS hosted database if that is more appropriate). Create a bucket in Amazon S3 such as my-bigdata-project-XX (where XX are your initials). Within the bucket, create folders for landing, raw, trusted, curated and models.  Your data files should end up in the landing folder for this milestone.
Document the code, commands and steps you used to extract and collect the data.  If you are collecting data from an API, show the code used. If you are downloading the data from a site, document the commands used to download the data. The downloading process should be able to be automated (scripted) in code and repeatable. The data should not be downloaded to your own computer. Add a new section to your project document with all of the above details. You can show a screen shot (picture) of your Amazon S3 bucket and landing folder to demonstrate that you have the data downloaded. If you have extensive code examples, place these in an Appendix and just give the main points in the body of the project report.

Milestone 3 Due 10/20/2023 (15 points)
Exploratory Data Analysis: Write the Python, PySpark or SparkSQL codes to load the data set from Amazon S3 and produce descriptive statistics about the data.  Use your EC2 instance and the Boto3 Python module or a cluster. At a minimum, this should include the number of observations (records, images, reviews, documents, etc.), list of variables (fields or columns), number of missing (null or N/A) fields in the observations, and the min/max/avg/stdev for all numeric variables. For date variables, include min and max dates. For image data include the number of images in each category and the min/max/average height and width of the images. For reviews, tweets, comments and other text data, include the number of each document and statistics about the number of words in the documents.  You may create graphs / charts to show the distribution of data (e.g., histograms for categorical variables). Complete this section with a brief paragraph summarizing the data and any challenges you believe you will have in cleaning and feature engineering. In your code examples, please obscure any private keys (such as for kaggle.json and your AWS access key and secret key).  Copy your EDA source code into an Appendix and provide the highlights and conclusions of your work in the main part of the report.

Milestone 4 Due 11/17/2023 (30 points)
Feature Engineering and Modeling:  Write the PySpark code to read and process this data using an AWS EMR cluster. This will include code to read the source data, clean and normalize the data, feature engineering, training/testing and evaluation of the predictive models and output. Split your code into two parts: Cleaning and Modeling.
The Data Cleaning code will read data from /landing folder, apply the schema to the data, fill in nulls or remove records with nulls, remove unnecessary columns and then write the data to the /raw folder as a Parquet file.  Copy your data cleaning code to an Appendix.
The Modeling code will read data from the /raw folder, perform feature engineering, train/test split, modeling and evaluation. Data should be saved to the /trusted folder and models should be saved to the /models folder. Copy your feature engineering and modeling code into an Appendix
Results of the analysis should be written to a file (or a series of files). Include all source code and proper references to each of the libraries/modules you are using. The resulting code should be able to be automated (scripted). Complete this section with a brief paragraph summarizing the main steps your program takes and any challenges you may have encountered while cleaning and processing the data. Provide a summary of the outputs in the main part of the report.

Milestone 5  Due 12/1/2023 (15 points)
Data Visualizing:  Create visualizations for the data and prediction results. Create at least four interesting visualizations that help you tell a data story and present the results.  These can be done in Python (e.g., Matplotlib, Seaborn etc.) or using a Visualization tool such as Tableau.  If possible, automate the visualizations portion of the pipeline.  Copy and paste screen shots of these visualizations into your project report document and include a few sentences of description for each of the visualizations. Include any Python codes for your visualization in an Appendix.

Milestone 6 Due 12/8/2023 (10 points)  
Summary and Conclusions:  Document the completed data processing pipeline and complete the project report with a summary of the project and the main conclusions you were able to draw from the data. Be sure to include citations for any code examples or other resources used. Include the GitHub URL for the shared project (see next milestone).

Milestone 7 Due 12/15/2023 (5 points)
Share the Project:  Post the project description and code on GitHub.  Include the URL  of your GitHub Project in the final report. Be sure to hide any security keys, passwords, credentials, etc. that may appear in your code.
